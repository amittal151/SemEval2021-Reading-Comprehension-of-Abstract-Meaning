# -*- coding: utf-8 -*-
"""MLMBert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13n_JBmCtHCySHY1sy1uEpyqlAnYxKJK2
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/My\ Drive/

# Commented out IPython magic to ensure Python compatibility.
# %cd CS779_shared_dataset/Task1/

import logging
import os
import argparse
import random
from tqdm import tqdm, trange
import csv

import numpy as np
import sys
import matplotlib.pyplot as plt

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler

from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
from pytorch_pretrained_bert.optimization import BertAdam
from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE

logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger(__name__)

# Arguments
max_seq_length = 450
train_batch_size = 32
eval_batch_size = 8       
learning_rate = 5e-5      
num_train_epochs = 3      # Epochs - less epochs to be used for BERT
warmup_proportion = 0.1   # How to use?
seed = 42                 # Random seed
local_rank = -1
optimize_on_cpu = True   # Whether to perform optimization and keep the optimizer averages on CPU
fp16 = False              # Whether to use 16-bit float precision instead of 32-bit
loss_scale = 128          # Loss scaling, positive power of 2 values can improve fp16 convergence
gradient_accumulation_steps = 4 

# Files : 
data_dir_1 = 'Task1/'
data_dir_2 = 'Task2/'
train_file_1 = 'Task1_train.csv'
dev_file_1 = 'Task1_dev.csv'
dev_file_1_full = 'Task_1_dev_full.csv'
dev_file_2_full = 'Task_2_dev_full.csv'


train_file_2 = 'Task2_train.csv'
dev_file_2 = 'Task2_dev.csv'
new_train_file_1 = 'New_Task1_train.csv'
new_train_file_2 = 'New_Task2_train.csv'
new_dev_file_1 = 'New_Task1_dev.csv'
new_dev_file_2 = 'New_Task2_dev.csv'

sum_train_file_1 = 'Task1_train_sum0.7.csv'
sum_train_file_2 = 'Task2_train_sum0.7.csv'
sum_dev_file_1 = 'Task1_dev_sum0.7.csv'


# Saved model files : 
train_on_article_1 = 'article_trained_task1.txt'
train_on_article_2 = 'article_train_2.txt'

path_modelsave_1 = 'model_file1.txt'
path_modelsave_2 = 'model_file2.txt'

modelfile_high_task1 = 'Models/model_high_acc_task1.txt'
modelfile_high_task2 = 'Models/model_high_acc_task2.txt'

sum_highacc_task1 = 'sum_high_acc_task1.txt'
sum_highacc_task2 = 'sum_high_acc_task2.txt'

non_sum_model_1 = 'model_file1_non_sum.txt'
non_sum_model_2 = 'model_file2_non_sum.txt'

no_cls_task1 = 'Models/no_cls_task1.txt'
no_cls_task2 = 'Models/no_cls_task2.txt'

more_trained_task1 = 'sir_suggestion_task1.txt'

def read_examples(input_file):

    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        lines = list(reader)

    examples = [
        {
            "article" : line[0],
            "question" : line[1],

            "options" : [line[2], line[3], line[4], line[5], line[6]],
         
            "label" : int(line[7]) 
         } for line in lines[1:]    # we skip the line with the column names
    ]
    return examples


def read_examples2(input_file):

    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        lines = list(reader)

    examples = [
        {
            "article" : line[0],
            "question" : line[1],

            "options" : [line[2], line[3], line[4], line[5], line[6]]

         } for line in lines[1:]    # we skip the line with the column names
    ]

    return examples




class InputFeatures(object):
    def __init__(self, choices_features, lm_labels, options, label):
        # We didn't stored tokens in features
        self.choices_features = [
            {
                'input_ids': input_ids,
                'input_mask': input_mask,
                'segment_ids': segment_ids
            }
            for _, input_ids, input_mask, segment_ids in choices_features 
        ]
        self.lm_labels = lm_labels
        self.options = options
        self.label = label


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
    """Truncates a sequence pair in place to the maximum length."""
    """Need to check whether truncation really helps, coz it might remove context! :( """

    # This is a simple heuristic which will always truncate the longer sequence
    # one token at a time. This makes more sense than truncating an equal percent
    # of tokens from each, since if one sequence is very short then each token
    # that's truncated likely contains more information than a longer sequence.
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_length:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()


def convert_examples_to_features(examples, tokenizer, max_seq_length):
    """Loads a data file into a list of `InputBatch`s."""
    
    features = []
    num_tokens_article = []
    count = 0
    masking_token_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]
    # padding_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
    for example_index, example in enumerate(examples):
        article_tokens = tokenizer.tokenize(example['article'])
        ques_tokens = tokenizer.tokenize(example['question'].replace("@placeholder", "_"))
        masked_index = ques_tokens.index('_')
        ques_tokens[masked_index] = '[MASK]'
        
#        options = example['options']
        options = tokenizer.convert_tokens_to_ids(example['options'])
#        print(options, '\n')

        choices_features = []
        _truncate_seq_pair(article_tokens, ques_tokens, max_seq_length - 3)

#        tokens = ["[CLS]"] + ques_tokens + ["[SEP]"] + article_tokens + ["[SEP]"]
        tokens = ques_tokens + ["[SEP]"] + article_tokens + ["[SEP]"]

#        segment_ids = [0] * (len(ques_tokens) + 2) + [1] * (len(article_tokens) + 1)
        segment_ids = [0] * (len(ques_tokens) + 1) + [1] * (len(article_tokens) + 1)

        input_ids = tokenizer.convert_tokens_to_ids(tokens)
        input_mask = [1] * len(input_ids)

        # Zero-pad up to the sequence length.
        padding = [0] * (max_seq_length - len(input_ids))
        input_ids += padding
        input_mask += padding
        segment_ids += padding
	
        candidates = example['options']
        candidates_ids = tokenizer.convert_tokens_to_ids(candidates)
#        print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example['options'][example['label']]))[0])

        lm_labels = [-1 if t_id != masking_token_id else tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example['options'][example['label']]))[0] for t_id in input_ids]
#        lm_labels = [t_id if t_id != 0 else -1 for t_id in input_ids]
#        for i, t_id in enumerate(lm_labels) :
#            if(t_id == masking_token_id):
#                lm_labels[i] = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example['options'][example['label']]))[0]

#        print(lm_labels, '\n')
        # tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example['options'][example['label']]))[0]
        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(segment_ids) == max_seq_length
        assert len(lm_labels) == max_seq_length

        choices_features.append((tokens, input_ids, input_mask, segment_ids))

        features.append(
            InputFeatures(
                choices_features = choices_features,
                lm_labels = lm_labels,
                options = options,
                label = example['label']
            )
        )
    
    return features



def copy_optimizer_params_to_model(named_params_model, named_params_optimizer):
    """ Utility function for optimize_on_cpu and 16-bits training.
        Copy the parameters optimized on CPU/RAM back to the model on GPU
    """
    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):
        if name_opti != name_model:
            logger.error("name_opti != name_model: {} {}".format(name_opti, name_model))
            raise ValueError
        param_model.data.copy_(param_opti.data)

        
def set_optimizer_params_grad(named_params_optimizer, named_params_model, test_nan=False):
    """ Utility function for optimize_on_cpu and 16-bits training.
        Copy the gradient of the GPU parameters to the CPU/RAMM copy of the model
    """
    is_nan = False
    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):
        if name_opti != name_model:
            logger.error("name_opti != name_model: {} {}".format(name_opti, name_model))
            raise ValueError
        if param_model.grad is not None:
            if test_nan and torch.isnan(param_model.grad).sum() > 0:
                is_nan = True
            if param_opti.grad is None:
                param_opti.grad = torch.nn.Parameter(param_opti.data.new().resize_(*param_opti.data.size()))
            param_opti.grad.data.copy_(param_model.grad.data)
        else:
            param_opti.grad = None
    return is_nan



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#print(device)
n_gpu = torch.cuda.device_count()

train_batch_size = int(train_batch_size / gradient_accumulation_steps)
#Initialise seeds
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

if n_gpu > 0:
    torch.cuda.manual_seed_all(seed)


# ------------------------------------Get Tokenizer--------------------------------
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=False)


# -----------------------------------------Model------------------------------------
values = torch.load(no_cls_task2)

model = BertForMaskedLM.from_pretrained('bert-large-uncased')

checkpoint = True
if checkpoint :
    model.load_state_dict(values['model'])


if fp16:
    model.half()

if n_gpu > 1:
    model = torch.nn.DataParallel(model)

model.to(device)

# Get the parameters of the model. 
# All the hidden weights of the model

if fp16:
    param_optimizer = [(n, param.clone().detach().to('cpu').float().requires_grad_()) \
                        for n, param in model.named_parameters()]
elif optimize_on_cpu:
    param_optimizer = [(n, param.clone().detach().to('cpu').requires_grad_()) \
                        for n, param in model.named_parameters()]
else:
    param_optimizer = list(model.named_parameters())

no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}
    ]


train_examples = read_examples(data_dir_2 + train_file_2)
num_train_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)

t_total = num_train_steps

optimizer = BertAdam(optimizer_grouped_parameters,
                         lr = learning_rate,
                         warmup = warmup_proportion,
                         t_total = t_total)



# Define Accuracy check metrics and training utils

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

def select_field(features, field):
    return [
        
            feature.choices_features[0][field]
        
        for feature in features
    ]

def classifiction_metric(preds, labels, label_list):
    """ The Metric of classification, input should be numpy format """

    acc = metrics.accuracy_score(labels, preds)

    labels_list = [i for i in range(len(label_list))]

    report = metrics.classification_report(
        labels, preds, labels=labels_list, target_names=label_list, digits=5, output_dict=True)

    return acc, report



#Training STEP 
"""
global_step = 0
train_features = convert_examples_to_features(
        train_examples, tokenizer, max_seq_length
        )


logger.info("***** Running training *****")
logger.info("  Num examples = %d", len(train_examples))
logger.info("  Batch size = %d", train_batch_size)
logger.info("  Num steps = %d", num_train_steps)

all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)
all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)
all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)
all_options = torch.tensor([f.options for f in train_features], dtype=torch.long)
all_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)
all_lm_labels = torch.tensor([f.lm_labels for f in train_features], dtype=torch.long)


train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_options, all_labels, all_lm_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)

model.train()

correct_ans = 0
wrong_list = []
total_ans = 0

for _ in trange(int(num_train_epochs), desc="Epoch"):
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    # all_preds = np.array([], dtype=int)
    # all_labels = np.array([], dtype=int)

    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        batch = tuple(t.to(device) for t in batch)
       	input_ids, input_mask, segment_ids, options, label, lm_label_ids = batch
        # print(input_ids.size(), input_mask.size())
        # sys.exit()
        # try :
        loss = model(input_ids, segment_ids, input_mask, lm_label_ids)              
     
        if n_gpu > 1:
            loss = loss.mean() # mean() to average on multi-gpu.
        if fp16 and loss_scale != 1.0:
            # rescale loss for fp16 training
            # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html
            loss = loss * loss_scale
        if gradient_accumulation_steps > 1:
            loss = loss / gradient_accumulation_steps
        loss.backward()
        tr_loss += loss.item()
        nb_tr_examples += input_ids.size(0)
        nb_tr_steps += 1
        if (step + 1) % gradient_accumulation_steps == 0:
            if fp16 or optimize_on_cpu:
                if fp16 and loss_scale != 1.0:
                    # scale down gradients for fp16 training
                    for param in model.parameters():
                        if param.grad is not None:
                            param.grad.data = param.grad.data / loss_scale
                is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)
                if is_nan:
                    logger.info("FP16 TRAINING: Nan in gradients, reducing loss scaling")
                    loss_scale = loss_scale / 2
                    model.zero_grad()
                    continue
                optimizer.step()
                copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)
            else:
                optimizer.step()

            train_loss = tr_loss / nb_tr_steps
            print("\tTraining loss : ", train_loss)

            model.zero_grad()
            global_step += 1
            
          
torch.save({
    'model': model.module.state_dict()
}, no_cls_task2)
   
     
"""

#EVALUATION begins!!!

model.eval()

correct_score = 0
wrong_list = []
examples = read_examples2('task1_test.csv')
#examples = examples[:200]

for idx, example in enumerate(examples) :
    article = example['article']
    text = example['question']
    text = text.replace("@placeholder", "_")
    tokenized_question = tokenizer.tokenize(text)
    masked_index = tokenized_question.index('_')
    tokenized_question[masked_index] = "[MASK]"
    tokenized_article = tokenizer.tokenize(article)
    tokens = tokenized_question + ["[SEP]"] + tokenized_article
#    tokens = ['[CLS]'] + tokenized_question + ["[SEP]"] + tokenized_article    

    if(len(tokens) >= max_seq_length-1):
        tokens = tokens[:max_seq_length-1]
	
    tokens += ["[SEP]"]
        

    candidates = example['options']
    candidates_ids = tokenizer.convert_tokens_to_ids(candidates)

    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)
    
    segments_ids = [0] * (len(tokenized_question) + 1) + [1] * (len(tokens) - len(tokenized_question) - 1)
#    segments_ids = [0] * (len(tokenized_question) + 2) + [1] * (len(tokens) - len(tokenized_question) - 2)	
 
    input_mask = [1] * len(indexed_tokens)

    # Zero-pad up to the sequence length.
    padding = [0] * (max_seq_length - len(indexed_tokens))
    indexed_tokens += padding
    input_mask += padding
    segments_ids += padding
    
    # if(len(tokens) > max_seq_length):
    #     segments_ids = segments_ids[:max_seq_length]

    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])
    mask_tensors = torch.tensor([input_mask])

    predictions = model(tokens_tensor, segments_tensors, mask_tensors)

    predictions_candidates = predictions[0, masked_index, candidates_ids]
    answer_idx = torch.argmax(predictions_candidates).item()
    print(answer_idx)
"""
    print("Index : ", idx, "\nConfidence Score : ", predictions_candidates)
    print("Correct answer : ", answer_idx, "\tLabel :", example['label'], '\n') 
    if(answer_idx == example['label']):
        correct_score += 1
    else :
#        c_sum = 0
#        for i in predictions_candidates:
#            c_sum += i
#        confidence = [item/c_sum for item in predictions_candidates]
        wrong_list.append(idx)
#            'idx' : idx,
#            'confidence' : confidence
#        })

    # print(f'The most likely word is "{candidates[answer_idx]}".')

accuracy = correct_score / len(examples)
print("Accuracy :", accuracy)
print("Correct answers :", correct_score) 

print("wrong list items :\n", wrong_list) 
"""
#print("Wrong Answers are :")
#for i in wrong_list:
#    print("Index :", i['idx'])
#    print("Confidence :", i['confidence'])
#    print('\n')
 

# print (correct_score)
# print(correct_score/ (correct_score + len(wrong_list)))

# my_test_examples = read_examples(train_file_1)
# for e in my_test_examples :
#     for option in e['options']:
#         tokens = tokenizer.tokenize(option)
#         indexes = tokenizer.convert_tokens_to_ids(tokens)

# print(tokenizer.tokenize('commemorative'))

# print(tokenizer.convert_tokens_to_ids(['[MASK]']))

